{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r\"C:\\Users\\gonca\\Documents\\GitHub\\Portefolio_SI\\src\")\n",
    "\n",
    "from iobla.csv_file import read_csv\n",
    "from data.dataset import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1\n",
    "dataset = read_csv(r\"..\\datasets\\iris\\iris.csv\", sep = \",\", features = \"True\", label = True)\n",
    "dataset2 = read_csv(r\"..\\datasets\\iris\\iris_missing_data.csv\", sep = \",\", features = \"True\", label = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2\n",
    "len(dataset.X[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.45, 3.03, 5.33, 2.17])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.3\n",
    "temp = dataset.X[-10: , : ]\n",
    "Dataset(temp).get_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.4 \n",
    "temp = dataset.X <= 6  \n",
    "final = dataset.X[np.all(temp, axis=1)] \n",
    "final.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.5\n",
    "dataset.X[dataset.y != 'Iris-setosa'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris dataset: (150, 4)\n",
      "Iris_missing dataset (150, 4)\n",
      "\n",
      "......... dropna being applied to both datasets ........\n",
      "\n",
      "Iris dataset after dropna method: (150, 4)\n",
      "Iris_missing dataset after dropna method: (134, 4)\n"
     ]
    }
   ],
   "source": [
    "#2.1\n",
    "from data.dataset import *\n",
    "\n",
    "print(\"Iris dataset:\", dataset.X.shape)\n",
    "print(\"Iris_missing dataset\", dataset2.X.shape)\n",
    "print()\n",
    "print(\"......... dropna being applied to both datasets ........\")\n",
    "dataset.dropna()\n",
    "dataset2.dropna()\n",
    "print()\n",
    "print(\"Iris dataset after dropna method:\", dataset.X.shape)\n",
    "print(\"Iris_missing dataset after dropna method:\", dataset2.X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nº de valores nulos no Iris dataset: 0\n",
      "Nº de valores nulos no Iris_missing dataset 17\n",
      "\n",
      "......... fillna being applied to both datasets using the mean ........\n",
      "\n",
      "Nº de valores nulos no Iris dataset Iris dataset after fillna method: 0\n",
      "Nº de valores nulos no Iris dataset Iris_missing dataset after fillna method: 0\n"
     ]
    }
   ],
   "source": [
    "# 2.2\n",
    "print(\"Nº de valores nulos no Iris dataset:\", np.sum(np.isnan(dataset.X)))\n",
    "print(\"Nº de valores nulos no Iris_missing dataset\", np.sum(np.isnan(dataset2.X)))\n",
    "print()\n",
    "print(\"......... fillna being applied to both datasets using the mean ........\")\n",
    "dataset.fillna(\"mean\")\n",
    "dataset2.fillna(\"mean\")\n",
    "print()\n",
    "print(\"Nº de valores nulos no Iris dataset Iris dataset after fillna method:\", np.sum(np.isnan(dataset.X)))\n",
    "print(\"Nº de valores nulos no Iris dataset Iris_missing dataset after fillna method:\", np.sum(np.isnan(dataset2.X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris dataset: (150, 4)\n",
      "Iris dataset sample 1: [5.1 3.5 1.4 0.2]\n",
      "Iris dataset sample 2: [4.9 3.  1.4 0.2]\n",
      "\n",
      "......... remove_by_index being applied ........\n",
      "\n",
      "Iris dataset after remove_by_index: (149, 4)\n",
      "Iris dataset new sample 1: [4.9 3.  1.4 0.2]\n"
     ]
    }
   ],
   "source": [
    "#2.3 \n",
    "print(\"Iris dataset:\", dataset.X.shape)\n",
    "print(\"Iris dataset sample 1:\", dataset.X[0])\n",
    "print(\"Iris dataset sample 2:\", dataset.X[1])\n",
    "print()\n",
    "print(\"......... remove_by_index being applied ........\")\n",
    "print()\n",
    "dataset.remove_by_index(0)\n",
    "print(\"Iris dataset after remove_by_index:\", dataset.X.shape)\n",
    "print(\"Iris dataset new sample 1:\", dataset.X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selection.select_k_best import SelectKBest\n",
    "from feature_selection.variance_threshold import VarianceThreshold\n",
    "from feature_selection.select_percentile import SelectPercentile\n",
    "from statisticsbla.f_classification import f_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset intial features: Index(['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], dtype='object')\n",
      "\n",
      "Initializing SelectPercentile with 50% and f_classification\n",
      "\n",
      ".............. Calculating scores .............\n",
      "[ 119.26450218   47.3644614  1179.0343277   959.32440573]\n",
      "[1.66966919e-31 1.32791652e-16 3.05197580e-91 4.37695696e-85]\n",
      "\n",
      ".............. Transforming dataset .............\n",
      "Transformed dataset features: ['petal_length', 'petal_width']\n"
     ]
    }
   ],
   "source": [
    "#3.3\n",
    "print(\"dataset intial features:\", dataset.features)\n",
    "print()\n",
    "print(\"Initializing SelectPercentile with 50% and f_classification\")\n",
    "selector = SelectPercentile(f_classification, percentile=50)\n",
    "selector = selector.fit(dataset)\n",
    "print()\n",
    "print(\".............. Calculating scores .............\")\n",
    "print(selector.F)\n",
    "print(selector.p)\n",
    "print()\n",
    "new_dataset = selector.transform(dataset)\n",
    "print(\".............. Transforming dataset .............\")\n",
    "selector.fit_transform(new_dataset)\n",
    "print(\"Transformed dataset features:\", new_dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.\n",
    "# manhattan_distance.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> My PCA implementation:\n",
      "\n",
      "Explained variance: [4.19667516 0.24062861 0.07800042]\n",
      "\n",
      "Explained variance ratio: [0.92461621 0.05301557 0.01718514]\n",
      "\n",
      "Components: [[ 0.36158968 -0.65653988  0.58099728]\n",
      " [-0.08226889 -0.72971237 -0.59641809]\n",
      " [ 0.85657211  0.1757674  -0.07252408]\n",
      " [ 0.35884393  0.07470647 -0.54906091]]\n",
      "\n",
      "---------------------------------------------------------------------\n",
      " --> SKlearn PCA:\n",
      "\n",
      "Explained variance: [4.22484077 0.24224357 0.07852391]\n",
      "\n",
      "Explained variance ratio: [0.92461621 0.05301557 0.01718514]\n",
      "\n",
      "Components: [[ 0.36158968 -0.08226889  0.85657211  0.35884393]\n",
      " [ 0.65653988  0.72971237 -0.1757674  -0.07470647]\n",
      " [-0.58099728  0.59641809  0.07252408  0.54906091]]\n"
     ]
    }
   ],
   "source": [
    "# 5.2\n",
    "print(\" --> My PCA implementation:\")\n",
    "print()\n",
    "from decomposition.pca import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit_transform(dataset.X)\n",
    "print(\"Explained variance:\", pca.explained_variance)\n",
    "print()\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio)\n",
    "print()\n",
    "print(\"Components:\", pca.components)\n",
    "\n",
    "from sklearn.decomposition import PCA as sklearnpca\n",
    "print()\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\" --> SKlearn PCA:\")\n",
    "SKlearn_pca = sklearnpca(n_components=3)\n",
    "SKlearn_pca.fit(dataset.X)\n",
    "print()\n",
    "print(\"Explained variance:\", SKlearn_pca.explained_variance_)\n",
    "print()\n",
    "print(\"Explained variance ratio:\", SKlearn_pca.explained_variance_ratio_)\n",
    "print()\n",
    "print(\"Components:\", SKlearn_pca.components_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................ Splitting the Iris dataset (train) ...........................\n",
      "\n",
      "Number of samples belonging to the 3 different classes:\n",
      "Iris-setosa: 41\n",
      "Iris-versicolor: 38\n",
      "Iris-virginica: 41\n",
      "\n",
      "................ Splitting the Iris dataset using stratified splitting (train) ..............................\n",
      "\n",
      "Number of samples belonging to the 3 different classes:\n",
      "Iris-setosa: 40\n",
      "Iris-versicolor: 40\n",
      "Iris-virginica: 40\n",
      "\n",
      "................ Splitting the Iris dataset (test) ...........................\n",
      "\n",
      "Number of samples belonging to the 3 different classes:\n",
      "Iris-setosa: 9\n",
      "Iris-versicolor: 12\n",
      "Iris-virginica: 9\n",
      "\n",
      "................ Splitting the Iris dataset using stratified splitting (test) ..............................\n",
      "\n",
      "Number of samples belonging to the 3 different classes:\n",
      "Iris-setosa: 10\n",
      "Iris-versicolor: 10\n",
      "Iris-virginica: 10\n"
     ]
    }
   ],
   "source": [
    "# 6.2 - Implementing stratified splitting\n",
    "\n",
    "from model_selection.split import train_test_split\n",
    "from model_selection.stratified_train_test_split import stratified_train_test_split\n",
    "\n",
    "dataset_train, dataset_test = train_test_split(dataset, test_size=0.2, random_state=50)\n",
    "print(\"................ Splitting the Iris dataset (train) ...........................\")\n",
    "print()\n",
    "print(\"Number of samples belonging to the 3 different classes:\")\n",
    "print(\"Iris-setosa:\", np.sum(dataset_train.y == \"Iris-setosa\"))\n",
    "print(\"Iris-versicolor:\", np.sum(dataset_train.y == \"Iris-versicolor\"))\n",
    "print(\"Iris-virginica:\", np.sum(dataset_train.y == \"Iris-virginica\"))\n",
    "print()\n",
    "\n",
    "dataset_train1, dataset_test1 = stratified_train_test_split(dataset, test_size=0.2, random_state=50)\n",
    "print(\"................ Splitting the Iris dataset using stratified splitting (train) ..............................\")\n",
    "print()\n",
    "print(\"Number of samples belonging to the 3 different classes:\")\n",
    "print(\"Iris-setosa:\", np.sum(dataset_train1.y == \"Iris-setosa\"))\n",
    "print(\"Iris-versicolor:\", np.sum(dataset_train1.y == \"Iris-versicolor\"))\n",
    "print(\"Iris-virginica:\", np.sum(dataset_train1.y == \"Iris-virginica\"))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"................ Splitting the Iris dataset (test) ...........................\")\n",
    "print()\n",
    "print(\"Number of samples belonging to the 3 different classes:\")\n",
    "print(\"Iris-setosa:\", np.sum(dataset_test.y == \"Iris-setosa\"))\n",
    "print(\"Iris-versicolor:\", np.sum(dataset_test.y == \"Iris-versicolor\"))\n",
    "print(\"Iris-virginica:\", np.sum(dataset_test.y == \"Iris-virginica\"))\n",
    "print()\n",
    "\n",
    "dataset_train1, dataset_test1 = stratified_train_test_split(dataset, test_size=0.2, random_state=50)\n",
    "print(\"................ Splitting the Iris dataset using stratified splitting (test) ..............................\")\n",
    "print()\n",
    "print(\"Number of samples belonging to the 3 different classes:\")\n",
    "print(\"Iris-setosa:\", np.sum(dataset_test1.y == \"Iris-setosa\"))\n",
    "print(\"Iris-versicolor:\", np.sum(dataset_test1.y == \"Iris-versicolor\"))\n",
    "print(\"Iris-virginica:\", np.sum(dataset_test1.y == \"Iris-virginica\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 - Add the RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 - Add the \"KNNRegressor\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rmse of the model is: (112.46586654719458+0j)\n"
     ]
    }
   ],
   "source": [
    "# 7.3 - Test the \"KNNRegressor\" class using the \"cpu.csv\"\n",
    "\n",
    "cpu_dataset = read_csv(r\"..\\datasets\\cpu.csv\", sep = \",\", features = \"True\", label = True)\n",
    "from models.knn_regressor import KNNRegressor\n",
    "from model_selection.split import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cpu_dataset_train, cpu_dataset_test = train_test_split(cpu_dataset, test_size=0.2)\n",
    "\n",
    "knn = KNNRegressor(k=6)\n",
    "\n",
    "knn.fit(cpu_dataset_train)\n",
    "\n",
    "score = knn.score(cpu_dataset_test)\n",
    "print(f'The rmse of the model is: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Optional - Categorical Naïve-Bayes (ver)\n",
    "\n",
    "from models.categorical_nb import CategoricalNB\n",
    "\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "\n",
    "# Generate binary features\n",
    "X = np.random.choice([0, 1], size=(n_samples, n_features))\n",
    "\n",
    "# Generate binary labels (0 or 1)\n",
    "y = np.random.choice([0, 1], size=n_samples)\n",
    "\n",
    "dataset = Dataset(X, y)\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "classifier = CategoricalNB(smoothing=1.0)\n",
    "classifier.fit(train_data)\n",
    "\n",
    "predictions = classifier.predict(test_data)\n",
    "accuracy = classifier.score(test_data)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test RidgeRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 - Implementing RidgeRegression with Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1 - Implementing the RandomForestClassifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 - Test the random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 - Implementing the StackingClassifier ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.2 - Test the StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the k_fold_cross_validation u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the grid_search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.1 - Implementing the randomized_search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11.2 - Test the randomized_search_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from neural_networks.layers import *\n",
    "from neural_networks.optimizers import *\n",
    "from neural_networks.losses import *\n",
    "from metrics import accuracy\n",
    "from neural_networks.neural_network import NeuralNetwork\n",
    "from neural_networks.activation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
